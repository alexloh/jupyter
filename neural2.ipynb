{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b27b6a5-0e56-4237-aae5-c695b344f05d",
   "metadata": {},
   "source": [
    "2nd attempt at making my own simple neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea9b75a-7b10-4dad-92a9-a9e8a6ba9072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 z = [[-0.3  0.7]\n",
      " [ 0.2  0.2]\n",
      " [ 0.7 -0.3]]\n",
      "layer1 = [[0.  0.7]\n",
      " [0.2 0.2]\n",
      " [0.7 0. ]]\n",
      "output z = [[-0.04]\n",
      " [ 0.94]\n",
      " [ 0.03]]\n",
      "output = [[0.  ]\n",
      " [0.94]\n",
      " [0.03]]\n",
      "\n",
      "Neural network with 1 hidden layer of size 2\n",
      "Current loss: 0.0014999999999999985\n",
      "Inputs: [[0. ]\n",
      " [0.5]\n",
      " [1. ]]\n",
      "Layer: [[0.  0.7]\n",
      " [0.2 0.2]\n",
      " [0.7 0. ]]\n",
      "Outputs: [[0.  ]\n",
      " [0.94]\n",
      " [0.03]]\n",
      "Weights1: [[ 1 -1]]\n",
      "Weights2: [[-3.1]\n",
      " [-3.2]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'method' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 107\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# for i in range(1):\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#     nn.backpropagate()\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#     nn.feedforward()\u001b[39;00m\n\u001b[1;32m    105\u001b[0m nn\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    109\u001b[0m plot(nn)\n",
      "Cell \u001b[0;32mIn[8], line 70\u001b[0m, in \u001b[0;36mNeuralNet.test\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer1 z = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m     layer1 \u001b[38;5;241m=\u001b[39m activation(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias1)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer1 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'method' and 'int'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    if x > 0: return 1\n",
    "    else: return 0\n",
    "\n",
    "activation = relu\n",
    "d_activation = relu_derivative\n",
    "\n",
    "class NeuralNet:\n",
    "    # What is the type of x and y???\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.input = self.x\n",
    "        # self.weights1 = np.random.rand(x.shape[1], 2) - 0.5\n",
    "        # self.bias1 = np.zeros(2)\n",
    "        # self.weights2 = np.random.rand(2, 1) - 0.5\n",
    "        # self.bias2 = np.zeros(1)\n",
    "        self.weights1 = np.array([[1, -1]])\n",
    "        self.bias1 = np.array([[-0.3, 0.7]])\n",
    "        self.weights2 = np.array([[-3.1], [-3.2]])\n",
    "        self.bias2 = np.array([[2.2]])\n",
    "        self.output = np.zeros(self.y.shape)\n",
    "        self.learning_rate = 1\n",
    "\n",
    "    def feedforward(self):\n",
    "        print(f\"layer1 z = {np.dot(self.input, self.weights1) + self.bias1}\")\n",
    "        layer1 = activation(np.dot(self.input, self.weights1) + self.bias1)\n",
    "        self.layer1 = layer1\n",
    "        print(f\"layer1 = {self.layer1}\")\n",
    "        print(f\"output z = {np.dot(self.layer1, self.weights2) + self.bias2}\")\n",
    "        output = activation(np.dot(layer1, self.weights2) + self.bias2)\n",
    "        self.output = output\n",
    "        print(f\"output = {self.output}\")\n",
    "\n",
    "    def backpropagate(self):\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * d_activation(self.output)))\n",
    "        d_bias2 = np.sum(2*(self.y - self.output) * d_activation(self.output), axis=0)\n",
    "        print(f\"d_bias2 = {d_bias2}\")\n",
    "        print(f\"bias2 = {self.bias2}\")\n",
    "        #d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * d_activation(self.output), self.weights2.T) * d_activation(self.layer1)))\n",
    "        #d_bias1 = (np.dot(2*(self.y - self.output) * d_activation(self.output), self.weights2.T) * d_activation(self.layer1))\n",
    "        #print(f\"d_bias1 = {d_bias1}\")\n",
    "        #self.weights1 += d_weights1 * self.learning_rate\n",
    "        #self.bias1 += d_bias1 * self.learning_rate\n",
    "        self.weights2 += d_weights2 * self.learning_rate\n",
    "        self.bias2 += d_bias2 * self.learning_rate\n",
    "\n",
    "    def display(self):\n",
    "        print()\n",
    "        print(\"Neural network with 1 hidden layer of size 2\")\n",
    "        print(f\"Current loss: {self.loss()}\")\n",
    "        print(f\"Inputs: {self.input}\")\n",
    "        print(f\"Layer: {self.layer1}\")\n",
    "        print(f\"Outputs: {self.output}\")\n",
    "        print(f\"Weights1: {self.weights1}\")\n",
    "        print(f\"Weights2: {self.weights2}\")\n",
    "\n",
    "    def test(self, inputs):\n",
    "        print(f\"layer1 z = {np.dot(input, self.weights1) + self.bias1}\")\n",
    "        layer1 = activation(np.dot(input, self.weights1) + self.bias1)\n",
    "        print(f\"layer1 = {self.layer1}\")\n",
    "        print(f\"output z = {np.dot(self.layer1, self.weights2) + self.bias2}\")\n",
    "        output = activation(np.dot(layer1, self.weights2) + self.bias2)\n",
    "        print(f\"output = {self.output}\")\n",
    "        return output\n",
    "\n",
    "    def loss(self):\n",
    "        return np.average((self.y - self.output) ** 2)\n",
    "\n",
    "    def mean_of_square_errors(predicted, actual):\n",
    "        if len(predicted) != len(actual):\n",
    "            sys.exit(\"mean_of_square_errors: Predict and actual are different length\")\n",
    "        sum = 0\n",
    "        for i in range(len(predicted)):\n",
    "            sum += (predicted[i] - actual[i]) ** 2\n",
    "        return sum / len(predicted)\n",
    "\n",
    "# Add function to plot shape of x vs y\n",
    "def plot(nn):\n",
    "    x = np.linspace(0, 1)\n",
    "    y = nn.feedforward(np.array(x)[np.newaxis].T, test=true)\n",
    "    plt.plot(x,y)\n",
    "\n",
    "x = np.array([ [0], [0.5], [1] ])\n",
    "y = np.array([ [0], [1], [0] ])\n",
    "\n",
    "nn = NeuralNet(x,y)\n",
    "\n",
    "nn.feedforward()\n",
    "# for i in range(1):\n",
    "#     nn.backpropagate()\n",
    "#     nn.feedforward()\n",
    "\n",
    "nn.display()\n",
    "\n",
    "print(nn.test(np.array([[0], [0.5], [1]])))\n",
    "\n",
    "plot(nn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
